


import com.datastax.spark.connector._
import org.apache.spark.SparkConf
import java.util._
import edu.stanford.nlp.pipeline._
import edu.stanford.nlp.ling._
import edu.stanford.nlp.ling.CoreAnnotations._
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations
import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations
import edu.stanford.nlp.simple._;
import scala.collection.JavaConversions._


//val conf = new SparkConf(true).set("spark.cassandra.connection.host", "172.31.62.42")
//val sc = new org.apache.spark.SparkContext(conf)
val rdd = sc.cassandraTable("dishes_db", "dishes")
rdd.count


//val sample = sqlContext.read.json("input/yelp_academic_dataset_review.json")
val sample = sqlContext.read.json("s3://roy-data/yelp_reviews.json")
//val reviews = sqlContext.read.json("input/yelp_academic_dataset_review.json")
val reviews = sample.sample(false,0.0001)
reviews.cache
//reviews.show

case class ReviewInfo(score:Int, promoText:String, promoDate:String, promoScore:Int) extends Product with Serializable
type ReviewCollector = (Int, ReviewInfo)
type DishScores = (String, (Int, ReviewInfo)) 

val createScoreCombiner = (review: ReviewInfo) => (1, review)

val scoreCombiner = (collector: ReviewCollector, review1: ReviewInfo) => {
    var promoScore = 0;
    var promoText = "";
    var promoDate = "";
    val (numReviews, (review2)) = collector
    if (review1.score > review2.score) { promoText = review1.promoText; promoScore = review1.score; promoDate = review1.promoDate; }
    else { promoText = review2.promoText; promoScore = review2.score; promoDate = review2.promoDate; }
    
    val reviewInfoResult = new ReviewInfo(review1.score + review2.score, promoText, promoDate, promoScore)
    (numReviews + 1, reviewInfoResult)
}

val scoreMerger = (collector1: ReviewCollector, collector2: ReviewCollector) => {
      
    val (numReviews1, (review1)) = collector1
    val (numReviews2, (review2)) = collector2

    var promoScore = 0;
    var promoText = "";
    var promoDate = "";

    if (review1.score > review2.score) { promoText = review1.promoText; promoScore = review1.score; promoDate = review1.promoDate; }
    else { promoText = review2.promoText; promoScore = review2.score; promoDate = review2.promoDate; }

    val reviewInfoResult = new ReviewInfo(review1.score + review2.score, promoText, promoDate, promoScore)
    (numReviews1 + numReviews2, reviewInfoResult)
}   

val averagingFunction = (dishScore: DishScores) => {
       val (key, (numReviews, reviewInfo)) = dishScore
       (key, (reviewInfo.score / numReviews), reviewInfo.score, numReviews, reviewInfo.promoText, reviewInfo.promoDate, reviewInfo.promoScore)
    }

def getScores (reviews: org.apache.spark.sql.DataFrame, items: scala.collection.mutable.Set[String]) = {
  val scores = reviews.mapPartitions( rows => {
  // Create core NLP
  val props = new Properties()
  props.put("annotators", "tokenize, ssplit, pos, lemma, parse, sentiment")
  val coreNLP = new StanfordCoreNLP(props, false)

  rows.map{ row => {

  val review = row.getAs[String]("text")
  val date = row.getAs[String]("date")
  val doc = new Document(review);
  var item = "";
  var score = -1;
  var sentenceWithItem = ""

  for (sentence <- doc.sentences()) {  
    
    for (lemma <- sentence.lemmas()) {  
            
           if (items.contains(lemma)){
              item = lemma;
           }
        }

        if (item != ""){
          //score = getSentiment(sentence.toString)
          sentenceWithItem = sentence.text
          val scoreArray = coreNLP.process(sentence.toString).
          get(classOf[CoreAnnotations.SentencesAnnotation]).
          map(_.get(classOf[SentimentCoreAnnotations.SentimentAnnotatedTree])).
          map(RNNCoreAnnotations.getPredictedClass(_))
          score = scoreArray(0) + 1
      }
    }

    val key = row.getAs[String]("business_id") + "SEPARATOR" + item
    val reviewInfo = new ReviewInfo(score, sentenceWithItem, date, score)
    (key, reviewInfo)
    }}
  })

  val filteredScores = scores.filter{ case(key, reviewInfo) => reviewInfo.score != -1}	
  val groupedScores = filteredScores.combineByKey(createScoreCombiner, scoreCombiner, scoreMerger)

  val averageScores = groupedScores.map(averagingFunction).
    map{ case(key, avgscore, totalScore, numReviews, promoText, promoDate, promoScore) => 
      val tokens = key.split("SEPARATOR"); (tokens(0), tokens(1), avgscore, totalScore, numReviews, promoText, promoScore)
    }

  //val step1 = scores.aggregateByKey((0.0, 0))((a, b) => (a._1 + b, a._2 + 1), (a, b) => (a._1 + b._1, a._2 + b._2))
  //val avgByKey = step1.mapValues(i => (i._1, i._2, i._1 * 1.0/i._2))
  //val dataToStore = avgByKey.map{ case(key, (total, num, myavg)) => val tokens = key.split("SEPARATOR"); (tokens(0), tokens(1), total, num, myavg, "", 0)}
  averageScores
}

val scores = getScores(sample, items)
//val test = scores.filter{ case(id, item, totalscore, numreviews, avgscore) => numreviews > 1}.take(20)
//test.foreach(println)
//  filteredScores.map{ case(id, item, score, review) => ((item + id), 1}.reduceByKey(_+_)

// Write to Cassandra
//val saveData = scores.takeSample(false, 20)
scores.saveToCassandra("dishes_db","dishes", 
  SomeColumns("restaurantid", "dish", "avgrating", "totalscore", "numreviews", "promo", "promorating"))

//ColumnDef(promo,RegularColumn,VarCharType), ColumnDef(promorating,RegularColumn,IntType)

